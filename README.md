## Large-Scale Representation Learning on Graphs via BootStrapping

This project provides an implementation of the [Bootstrapped Graph Latents (BGRL)](https://arxiv.org/pdf/2102.06514) model, a graph representation learning method that learns by predicting alternative augmentations of the input. It utilizes various graph neural network encoders and an MLP predictor for generating node embeddings and performing classification task on top of these embeddings.
### Setup

1. **Clone the repository**:
    ```sh
    git clone https://github.com/al3ssandrocaruso/BGRL.git
    cd BGRL
    ```

2. **Install dependencies**:
    Ensure you have Python 3.7+ installed. Then, run:
    ```sh
    pip install -r requirements.txt
    ```

### Available Datasets

- WikiCS
- Amazon Computers
- Amazon Photos
- Coauthor CS
- Cora

### Running the Script

1. **With Default Dataset (Coauthor_CS)**:
    Simply run:
    ```sh
    python runner.py
    ```

2. **With a Specific Dataset**:
    Use the `--dataset` argument to specify the dataset:
    ```sh
    python runner.py --dataset WikiCS
    ```

### Command-Line Arguments

- `--hidden_dim_encoder`: Dimension of hidden layers in the GCN encoder (default: 512)
- `--g_embedding_dim`: Dimension of the embedding generated by the encoder (default: 256)
- `--hidden_dim_predictor`: Dimension of hidden layers in the MLP predictor (default: 512)
- `--num_epochs`: Number of epochs for training (default: 100)
- `--early_stop_max`: Early stopping patience (default: 50)
- `--pf_view_1`: Probability of feature perturbation for the first view (default: 0.3)
- `--pf_view_2`: Probability of feature perturbation for the second view (default: 0.2)
- `--pe_view_1`: Probability of edge perturbation for the first view (default: 0.3)
- `--pe_view_2`: Probability of edge perturbation for the second view (default: 0.4)
- `--dataset`: Dataset to use (default: Coauthor_CS)
- `--optimizer`: Optimizer to use (`adam` or `sgd`, default: `adam`)
- `--encoder_type`: GNN Architecture (`GCN`, `GAT`, or `MPNN`, default: `GCN`)
- `--lr`: Learning rate (default: 1e-5)
- `--batch_norm`: Use batch normalization (`True` or `False`, default: `False`)
- `--layer_norm`: Use layer normalization (`True` or `False`, default: `False`)

Example:
```sh
python main.py --num_epochs 200 --early_stop_max 100 --dataset WikiCS
