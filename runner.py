import argparse
import torch
from torch import optim
from tqdm import tqdm
from sklearn.model_selection import train_test_split, GridSearchCV, ShuffleSplit
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from datasets import load_dataset
from models.loss import CosineSimilarityLoss
from models.encoder import GNNEncoder
from models.model import BGRL
from models.linearpredictor import LinearPredictor
from utils import augment_graph, print_memory_usage
import args as default_args


def parse_args():
    parser = argparse.ArgumentParser(description='Train and evaluate a BGRL model.')

    parser.add_argument('--hidden_dim_encoder', type=int, default=default_args.hidden_dim_encoder, help='Dimension of hidden layers in the GCN encoder (default: 512)')
    parser.add_argument('--g_embedding_dim', type=int, default=default_args.g_embedding_dim, help='Dimension of the embedding generated by the encoder (default: 256)')
    parser.add_argument('--hidden_dim_predictor', type=int, default=default_args.hidden_dim_predictor, help='Dimension of hidden layers in the MLP predictor (default: 512)')
    parser.add_argument('--num_epochs', type=int, default=default_args.num_epochs, help='Number of epochs for training (default: 100)')
    parser.add_argument('--pf_view_1', type=float, default=default_args.pf_view_1, help='Probability of feature perturbation for the first view (default: 0.3)')
    parser.add_argument('--pf_view_2', type=float, default=default_args.pf_view_2, help='Probability of feature perturbation for the second view (default: 0.2)')
    parser.add_argument('--pe_view_1', type=float, default=default_args.pe_view_1, help='Probability of edge perturbation for the first view (default: 0.3)')
    parser.add_argument('--pe_view_2', type=float, default=default_args.pe_view_2, help='Probability of edge perturbation for the second view (default: 0.4)')
    parser.add_argument('--dataset', type=str, choices=default_args.datasets, default=default_args.default_dataset, help='Dataset to use (default: Coauthor_CS)')
    parser.add_argument('--optimizer', type=str, choices=['adam', 'sgd'], default=default_args.optimizer, help='Optimizer to use (default: adam)')
    parser.add_argument('--encoder_type', type=str, choices=['GCN', 'GAT', 'MPNN'], default=default_args.encoder_type, help='GNN Architecture (default: GCN)')
    parser.add_argument('--lr', type=float, default=default_args.lr, help='Learning Rate (default: 1e-5)')
    parser.add_argument('--batch_norm', default=default_args.batch_norm, help='Use Batch Normalization (default: False)')
    parser.add_argument('--layer_norm', default=default_args.layer_norm, help='Use Layer Normalization (default: False)')

    return parser.parse_args()

def main():
    args = parse_args()

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    data = load_dataset(args.dataset).to(device)
    input_dim = data.x.size(1)

    gnn_encoder = GNNEncoder(input_dim, args.hidden_dim_encoder, args.g_embedding_dim, args.encoder_type, args.batch_norm, args.layer_norm)
    predictor = LinearPredictor(args.g_embedding_dim, args.hidden_dim_predictor, args.g_embedding_dim)
    model = BGRL(gnn_encoder, predictor).to(device)

    optimizer = optim.AdamW(model.parameters(), lr=args.lr) if args.optimizer == 'adam' else optim.SGD(model.parameters(), lr=args.lr)
    loss_fn = CosineSimilarityLoss()

    # Model Training
    model.train()
    for epoch in tqdm(range(args.num_epochs), desc="Training Epochs"):
        optimizer.zero_grad()

        g_a = augment_graph(data, args.pf_view_1, args.pe_view_1).to(device)
        g_b = augment_graph(data, args.pf_view_2, args.pe_view_2).to(device)

        target_a, prediction_a = model(g_a, g_b)
        target_b, prediction_b = model(g_b, g_a)

        loss = loss_fn([(prediction_a, target_a), (prediction_b, target_b)])
        loss.backward()
        optimizer.step()
        model.exp_moving_avg()

        if epoch == 0:
            print_memory_usage()

        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch + 1}/{args.num_epochs}, Loss: {loss.item()}")

    # Model Evaluation
    model.eval()
    with torch.no_grad():
        embeddings = model.get_trained_encoder()(data).cpu().numpy()
        labels = data.y.cpu().numpy()
    print("Graph embeddings created")

    X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, test_size=0.2, random_state=42)

    param_grid = {'C': [1e-5, 1e-4, 1e-3, 0.1, 1, 10, 100, 1000, 10000], 'solver': ['liblinear']}
    log_reg = LogisticRegression(penalty='l2', max_iter=10000)
    cross_validation = ShuffleSplit(n_splits=5, test_size=0.5, random_state=42)
    grid_search = GridSearchCV(log_reg, param_grid, cv=cross_validation, scoring='accuracy', n_jobs=-1)
    grid_search.fit(X_train, y_train)

    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)

    print(f"Best Hyperparameters: {grid_search.best_params_}")
    print(f"Accuracy: {accuracy}")

if __name__ == "__main__":
    main()
